{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediksi Sentimen: positif\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# 1. Definisikan arsitektur model persis sama dengan saat training.\n",
    "class IndoBERT_BiLSTM(nn.Module):\n",
    "    def __init__(self, bert_model=\"indobenchmark/indobert-base-p1\", lstm_hidden=128, num_classes=3):\n",
    "        super(IndoBERT_BiLSTM, self).__init__()\n",
    "        \n",
    "        # Load IndoBERT sebagai feature extractor.\n",
    "        self.bert = AutoModel.from_pretrained(bert_model)\n",
    "        self.bert.requires_grad_(False)  # Freeze bobot IndoBERT agar tidak berubah saat inferensi.\n",
    "        \n",
    "        # BiLSTM untuk memproses representasi vektor dari IndoBERT.\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=768, \n",
    "            hidden_size=lstm_hidden, \n",
    "            num_layers=2,\n",
    "            batch_first=True, \n",
    "            bidirectional=True, \n",
    "            dropout=0.3\n",
    "        )\n",
    "        \n",
    "        # Batch Normalization dan Dropout untuk membantu stabilitas.\n",
    "        self.batch_norm = nn.BatchNorm1d(lstm_hidden * 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Fully Connected layer untuk menghasilkan prediksi kelas.\n",
    "        self.fc = nn.Linear(lstm_hidden * 2, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Ekstraksi fitur dengan IndoBERT.\n",
    "        with torch.no_grad():\n",
    "            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Mengambil representasi token [CLS] (index 0) sebagai representasi kalimat.\n",
    "        bert_embedding = bert_output.last_hidden_state[:, 0, :]  # Bentuk: [batch_size, 768]\n",
    "        \n",
    "        # Proses dengan BiLSTM. Karena LSTM mengharapkan input 3 dimensi, kita tambahkan dimensi sekuens.\n",
    "        lstm_out, _ = self.lstm(bert_embedding.unsqueeze(1))  # Bentuk: [batch_size, seq_len=1, hidden_size*2]\n",
    "        lstm_out = lstm_out[:, -1, :]  # Ambil output terakhir dari LSTM.\n",
    "        \n",
    "        # Normalisasi dan dropout.\n",
    "        lstm_out = self.batch_norm(lstm_out)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Prediksi akhir.\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "\n",
    "# 2. Inisialisasi model dan device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = IndoBERT_BiLSTM().to(device)\n",
    "\n",
    "# 3. Load bobot model yang telah disimpan.\n",
    "# Pastikan file \"best_model.pth\" berada di direktori kerja Anda.\n",
    "model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
    "model.eval()  # Set model ke mode evaluasi.\n",
    "\n",
    "# 4. Load tokenizer yang sama dengan saat training.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "\n",
    "# 5. Buat mapping label.\n",
    "# Misalnya: {0: \"negatif\", 1: \"netral\", 2: \"positif\"}\n",
    "label_mapping = {0: \"negatif\", 1: \"netral\", 2: \"positif\"}\n",
    "\n",
    "# 6. Fungsi prediksi.\n",
    "def predict(text, model, tokenizer, device, label_mapping):\n",
    "    \"\"\"\n",
    "    Fungsi untuk melakukan prediksi sentimen pada input teks.\n",
    "    Args:\n",
    "        text (str): Komentar atau kalimat input.\n",
    "        model: Model yang telah diload.\n",
    "        tokenizer: Tokenizer IndoBERT.\n",
    "        device: Device yang digunakan (CPU atau GPU).\n",
    "        label_mapping (dict): Mapping indeks ke label (misal: 0->\"negatif\").\n",
    "    Returns:\n",
    "        str: Label sentimen prediksi.\n",
    "    \"\"\"\n",
    "    model.eval()  # Pastikan model dalam mode evaluasi.\n",
    "    \n",
    "    # Tokenisasi teks input.\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Lakukan inferensi tanpa gradien.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "    \n",
    "    # Ambil indeks prediksi dengan nilai tertinggi.\n",
    "    pred_idx = torch.argmax(outputs, dim=1).item()\n",
    "    return label_mapping[pred_idx]\n",
    "\n",
    "# 7. Contoh penggunaan prediksi.\n",
    "example_text = \"Produk ini sangat bagus dan berkualitas!\"\n",
    "prediction = predict(example_text, model, tokenizer, device, label_mapping)\n",
    "print(f\"Prediksi Sentimen: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediksi Sentimen: negatif\n"
     ]
    }
   ],
   "source": [
    "example_text = \"Produk ini sangat jelek\"\n",
    "prediction = predict(example_text, model, tokenizer, device, label_mapping)\n",
    "print(f\"Prediksi Sentimen: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediksi Sentimen: positif\n"
     ]
    }
   ],
   "source": [
    "example_text = \"Produk ini biasa \"\n",
    "prediction = predict(example_text, model, tokenizer, device, label_mapping)\n",
    "print(f\"Prediksi Sentimen: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"./test_comments_with_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil prediksi telah disimpan ke test_comments_with_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Load dataset CSV yang berisi komentar\n",
    "test_df = pd.read_csv(\"./dataset/comments.csv\")  # Ganti dengan path file Anda\n",
    "\n",
    "# Pastikan kolom \"comment\" ada di dataset\n",
    "if \"comment\" not in test_df.columns:\n",
    "    raise ValueError(\"Kolom 'comment' tidak ditemukan dalam dataset!\")\n",
    "\n",
    "# 2. Bersihkan dataset (pastikan semua komentar bertipe string)\n",
    "test_df[\"comment\"] = test_df[\"comment\"].astype(str)  # Konversi semua ke string\n",
    "test_df[\"comment\"] = test_df[\"comment\"].fillna(\"\")   # Jika ada NaN, ganti dengan string kosong\n",
    "\n",
    "# 3. Definisikan ulang fungsi prediksi dengan validasi input\n",
    "def predict(text, model, tokenizer):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"netral\"  # Label default jika komentar kosong atau tidak valid\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, attention_mask)\n",
    "    \n",
    "    pred_class = output.argmax(dim=1).item()\n",
    "    \n",
    "    mapping = {0: \"negatif\", 1: \"netral\", 2: \"positif\"}\n",
    "    return mapping[pred_class]\n",
    "\n",
    "# 4. Lakukan prediksi untuk setiap komentar di dataset\n",
    "predicted_labels = []\n",
    "for comment in test_df[\"comment\"]:\n",
    "    pred_label = predict(comment, model, tokenizer)\n",
    "    predicted_labels.append(pred_label)\n",
    "\n",
    "# 5. Tambahkan hasil prediksi ke dataset\n",
    "test_df[\"predicted_label\"] = predicted_labels\n",
    "\n",
    "# 6. Simpan hasil prediksi ke file CSV baru\n",
    "output_path = \"test_comments_with_predictions.csv\"\n",
    "test_df.to_csv(output_path, index=False)\n",
    "print(f\"Hasil prediksi telah disimpan ke {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no</th>\n",
       "      <th>comment</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Aku cuma pake sunscreen dan krim ini doang, al...</td>\n",
       "      <td>netral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Simpen dulu di keranjang kuning, nanti check out.</td>\n",
       "      <td>netral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Muka kusamku jadi segeran setelah rutin pake k...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Aku cuma pake sunscreen dan krim ini doang, al...</td>\n",
       "      <td>netral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Masih menjadi misteri kenapa Dr. Fay bisa seba...</td>\n",
       "      <td>netral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no                                            comment predicted_label\n",
       "0   1  Aku cuma pake sunscreen dan krim ini doang, al...          netral\n",
       "1   2  Simpen dulu di keranjang kuning, nanti check out.          netral\n",
       "2   3  Muka kusamku jadi segeran setelah rutin pake k...         positif\n",
       "3   4  Aku cuma pake sunscreen dan krim ini doang, al...          netral\n",
       "4   5  Masih menjadi misteri kenapa Dr. Fay bisa seba...          netral"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagus sekali, saya suka! -> positif\n",
      "Tidak bagus sama sekali -> negatif\n",
      "Biasa saja tidak terlalu buruk atau bagus -> netral\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 1. Siapkan kamus kata sentimen\n",
    "positive_words = {\"bagus\", \"baik\", \"indah\", \"cerah\", \"puas\", \"suka\", \"alhamdulillah\"}\n",
    "negative_words = {\"buruk\", \"jelek\", \"kecewa\", \"mengecewakan\"}\n",
    "negation_words = {\"tidak\", \"bukan\", \"kurang\", \"belum\"}\n",
    "\n",
    "def preprocess_negation(text):\n",
    "    \"\"\"\n",
    "    - Ubah ke huruf kecil dan ekstrak hanya kata (tanpa tanda baca)\n",
    "    - Jika menemukan kata negasi dan kata berikutnya ada dalam positive atau negative words,\n",
    "      maka tandai kata berikutnya dengan prefix \"NEG_\" (untuk positive) atau \"POS_\" (untuk negative)\n",
    "    - Jika tidak, proses kata seperti biasa\n",
    "    \"\"\"\n",
    "    # Ekstrak kata dengan regex agar tanda baca tidak ikut\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    new_words = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        word = words[i]\n",
    "        # Jika kata merupakan negasi dan ada kata setelahnya\n",
    "        if word in negation_words and i + 1 < len(words):\n",
    "            next_word = words[i+1]\n",
    "            # Jika kata berikutnya termasuk kata positif, tandai dengan \"NEG_\"\n",
    "            if next_word in positive_words:\n",
    "                new_words.append(\"NEG_\" + next_word)\n",
    "                i += 2  # Lewati kata negasi dan kata yang ditandai\n",
    "                continue\n",
    "            # Jika kata berikutnya termasuk kata negatif, tandai dengan \"POS_\"\n",
    "            elif next_word in negative_words:\n",
    "                new_words.append(\"POS_\" + next_word)\n",
    "                i += 2\n",
    "                continue\n",
    "            else:\n",
    "                # Jika kata berikutnya bukan kata sentimen, simpan kata negasi dan lanjutkan\n",
    "                new_words.append(word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "        i += 1\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def sentiment_score(text):\n",
    "    \"\"\"\n",
    "    Fungsi ini menghitung skor sentimen berdasarkan:\n",
    "      - Menambahkan +1 untuk setiap kata positif\n",
    "      - Mengurangi -1 untuk setiap kata negatif\n",
    "      - Untuk kata yang diberi prefix \"NEG_\", mengurangi -1 (karena kata positif yang dinetralkan)\n",
    "      - Untuk kata dengan prefix \"POS_\", menambahkan +1 (karena kata negatif yang dibalik artinya)\n",
    "    \"\"\"\n",
    "    processed_text = preprocess_negation(text)\n",
    "    score = 0\n",
    "    words = processed_text.split()\n",
    "    for word in words:\n",
    "        if word in positive_words:\n",
    "            score += 1\n",
    "        elif word in negative_words:\n",
    "            score -= 1\n",
    "        elif word.startswith(\"NEG_\"):\n",
    "            # Kata positif yang di-negate dihitung sebagai negatif\n",
    "            if word[4:] in positive_words:\n",
    "                score -= 1\n",
    "        elif word.startswith(\"POS_\"):\n",
    "            # Kata negatif yang di-negate dihitung sebagai positif\n",
    "            if word[4:] in negative_words:\n",
    "                score += 1\n",
    "    if score > 0:\n",
    "        return \"positif\"\n",
    "    elif score < 0:\n",
    "        return \"negatif\"\n",
    "    else:\n",
    "        return \"netral\"\n",
    "\n",
    "# Contoh pengujian:\n",
    "texts = [\n",
    "    \"Bagus sekali, saya suka!\",         # Seharusnya: positif\n",
    "    \"Tidak bagus sama sekali\",            # Seharusnya: negatif\n",
    "    \"Biasa saja tidak terlalu buruk atau bagus\"  # Seharusnya: netral\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print(f\"{text} -> {sentiment_score(text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomPred_num\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentiment_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpositif\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentiment_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnegatif\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomPred_num\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msentiment_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositif\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sentiment_score(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegatif\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[1;32mIn[20], line 50\u001b[0m, in \u001b[0;36msentiment_score\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msentiment_score\u001b[39m(text):\n\u001b[0;32m     43\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    Fungsi ini menghitung skor sentimen berdasarkan:\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m      - Menambahkan +1 untuk setiap kata positif\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m      - Untuk kata dengan prefix \"POS_\", menambahkan +1 (karena kata negatif yang dibalik artinya)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     processed_text \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_negation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     52\u001b[0m     words \u001b[38;5;241m=\u001b[39m processed_text\u001b[38;5;241m.\u001b[39msplit()\n",
      "Cell \u001b[1;32mIn[20], line 16\u001b[0m, in \u001b[0;36mpreprocess_negation\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m- Ubah ke huruf kecil dan ekstrak hanya kata (tanpa tanda baca)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m- Jika menemukan kata negasi dan kata berikutnya ada dalam positive atau negative words,\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m  maka tandai kata berikutnya dengan prefix \"NEG_\" (untuk positive) atau \"POS_\" (untuk negative)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m- Jika tidak, proses kata seperti biasa\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Ekstrak kata dengan regex agar tanda baca tidak ikut\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m words \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m())\n\u001b[0;32m     17\u001b[0m new_words \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     18\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "train_df['customPred_num'] = train_df['comment'].apply(lambda x: 2 if sentiment_score(x) == \"positif\" else (0 if sentiment_score(x) == \"negatif\" else 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
